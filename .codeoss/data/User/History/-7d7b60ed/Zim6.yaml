steps:
  # Step 1: Set up authentication with GCP using the service account keys
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -x  # Enable debug mode
        echo $SERVICE_ACCOUNT_KEY | base64 -d > /workspace/sa-key.json || { echo 'Decoding service account key failed'; exit 1; }
        gcloud auth activate-service-account --key-file=/workspace/sa-key.json || { echo 'Service account activation failed'; exit 1; }
        gcloud config set project mystical-sweep-439114-m6 || { echo 'Setting project failed'; exit 1; }

  # Step 2: Install Python dependencies
  - name: 'python:3.12'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pip install --upgrade pip
        pip install -r requirements.txt

  # Step 3: Deploy Airflow DAGs to Cloud Composer GCS bucket
  - name: 'gcr.io/cloud-builders/gsutil'
    args: ['-m', 'cp', '-r', 'etl_airflow/dags/*', 'gs://us-central1-my-composer-env-1f9279c3-bucket/dags/']

  # Step 4: Deploy the dbt project to GCS
  - name: 'gcr.io/cloud-builders/gsutil'
    args: ['-m', 'cp', '-r', 'dbt_project/*', 'gs://us-central1-my-composer-env-1f9279c3-bucket/data/dbt_project/']

  # Step 5: Trigger the dbt models in Cloud Composer
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'composer'
      - 'environments'
      - 'run'
      - 'my-composer-env'
      - '--location'
      - 'us-central1'
      - 'trigger_dag'
      - '--'
      - 'snowflake_to_gcs_and_bigquery_with_dbt'

substitutions:
  _PROJECT_ID: 'mystical-sweep-439114-m6'
  _SERVICE_ACCOUNT_KEY: '<base64-encoded-service-account-json>'
  _COMPOSER_BUCKET_NAME: 'us-central1-my-composer-env-1f9279c3-bucket'
  _COMPOSER_ENV_NAME: 'my-composer-env'

# Optional: Notification if the build fails
timeout: 1200s
